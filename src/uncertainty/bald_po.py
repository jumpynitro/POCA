"""
BALD-PO(x) = 0 for a deterministic model but numerical instabilities can lead to nonzero scores.

Cl = number of classes
K = number of model samples
N = number of examples
L = number of samples generated by the surrogate model

References:
    https://github.com/BlackHC/batchbald_redux/blob/master/01_batchbald.ipynb
"""

import math
import torch
from src.math import logmeanexp
from src.uncertainty.utils import check
from torch import Tensor
from typing import List, Sequence, Tuple, Union
import re


def entropy_from_logprobs(logprobs: Tensor) -> Tensor:
    """
    H[p(y|x)] = - ∑_{y} p(y|x) log p(y|x)

    Using torch.distributions.Categorical().entropy() would be cleaner but it uses lots of memory.

    Arguments:
        logprobs: Tensor[float], [*N, Cl]

    Returns:
        Tensor[float], [*N,]
    """
    return -torch.sum(torch.exp(logprobs) * logprobs, dim=-1)  # [*N,]


def entropy_from_probs(probs: Tensor) -> Tensor:
    """
    See entropy_from_logprobs.

    If p(y=y'|x) is 0, we make sure p(y=y'|x) log p(y=y'|x) evaluates to 0, not NaN.

    Arguments:
        probs: Tensor[float], [*N, Cl]

    Returns:
        Tensor[float], [*N,]
    """
    logprobs = torch.clone(probs)  #  [*N, Cl]
    logprobs[probs > 0] = torch.log(probs[probs > 0])  #  [*N, Cl]
    return -torch.sum(probs * logprobs, dim=-1)  # [*N,]


def po_entropy_from_logprobs(logprobs: Tensor, config_name: str) -> Tensor:
    """
    Arguments:
        logprobs: Tensor[float], [N, L, K, Cl]

    Returns:
        Tensor[float], [N,]
    """
    expectation_dims, entropy_dims = get_dim_config(config_name)
    if len(entropy_dims) > 0:
        logprobs = logmeanexp(logprobs, dim = entropy_dims)
    scores   = entropy_from_logprobs(logprobs)
    if len(expectation_dims) == 2:
        scores = torch.mean(scores, dim = expectation_dims)
    elif len(expectation_dims) == 1:
        scores = torch.mean(scores, dim = [1])        
    scores   = check(scores, math.log(logprobs.shape[-1]), score_type="ME")  # [N,]
    return scores  # [N,]

def po_entropy_from_probs(probs: Tensor, config_name: str) -> Tensor:
    """

    Arguments:
        probs: Tensor[float], [N, K, Cl]

    Returns:
        Tensor[float], [N,]
    """
    expectation_dims, entropy_dims = get_dim_config(config_name)
    if len(entropy_dims) > 0:
        probs = torch.mean(probs, dim = entropy_dims)
    scores = entropy_from_probs(probs)  # [N,]
    if len(expectation_dims) == 2:
        scores = torch.mean(scores, dim = expectation_dims)
    elif len(expectation_dims) == 1:
        scores = torch.mean(scores, dim = [1])
        
    scores = check(scores, math.log(probs.shape[-1]), score_type="ME")  # [N,]
    return scores  # [N,]

def get_dim_config(config_name: str) -> Union[List, List]:
    """
    Arguments:
        logprobs: Tensor[float], [N, L, K, Cl]

    Returns:
        Tensor[float], [N,]
    """
    if config_name == 'Exp_Entropy_unobs_par':
        expectation_dims, entropy_dims = [], [1, 2]
    elif config_name == 'Exp_par_Entropy_unobs':
        expectation_dims, entropy_dims = [2], [1]
    elif config_name == 'Exp_unobs_Entropy_par':
        expectation_dims, entropy_dims = [1], [2]
    elif config_name == 'Exp_unobs_par_Entropy':
        expectation_dims, entropy_dims = [1, 2], []
    return expectation_dims, entropy_dims

def obtain_scores(first_term, second_term, third_term, fourth_term, mode):
    
    all_scores = {}
    all_scores['Exp_Entropy_unobs_par'] = first_term
    all_scores['Exp_par_Entropy_unobs'] = second_term
    all_scores['Exp_unobs_Entropy_par'] = third_term
    all_scores['Exp_unobs_par_Entropy'] = fourth_term
    all_scores['first_MI']  = first_term - second_term
    all_scores['second_MI'] = third_term - fourth_term

    if mode == 'bald-po' or 'bald-po-feature' in mode:
        scores = third_term - fourth_term
    elif mode == 'bald-po-marginal':
        scores = first_term - second_term

    all_scores[mode] = scores

    return all_scores


def bald_po_from_logprobs(logprobs: Tensor, mode: str) -> Tensor:
    """
    BALD_PO(x) = E_{p(θ)}[H[p(y|x)] - H[p(y|x,θ)]] + E_{p(xu|x)p(θ)}[H[p(y|x, xu)] - H[p(y|x,θ, xu)]] 
               = H[p(y|x)] - E_{p(θ)}[H[p(y|x,θ)]] + E_{p(xu|x)H[p(y|x, xu)] - E_{p(xu|x)p(θ)}[H[p(y|x,θ, xu)]] 
               = H[E_{p(xu|x)p(θ)}[p(y|x,θ, xu)]] (1) - E_{p(θ)}[H[E_{p(xu|x)}[p(y|x,θ, xu)]]] (2)
               + E_{p(xu|x)}[H[E_{p(θ)}[p(y|x,θ, xu)]]] (3) - E_{p(xu)p(θ)}[H[p(y|x,θ, xu)]] (4)

    Arguments:
        logprobs: Tensor[float], [N, L, K, Cl]

    Returns:
        Tensor[float], [N,]
    """
    #marg_entropy = marginal_entropy_from_logprobs(logprobs)  # [N,]
    #cond_entropy = conditional_entropy_from_logprobs(logprobs)  # [N,]
    first_term  = po_entropy_from_logprobs(logprobs, 'Exp_Entropy_unobs_par')
    second_term = po_entropy_from_logprobs(logprobs, 'Exp_par_Entropy_unobs')
    third_term  = po_entropy_from_logprobs(logprobs, 'Exp_unobs_Entropy_par')
    fourth_term = po_entropy_from_logprobs(logprobs, 'Exp_unobs_par_Entropy')

    #scores = first_term - second_term + third_term - fourth_term  # [N,]
    #scores = third_term - fourth_term  # [N,]

    #scores = check(scores, math.log(logprobs.shape[-1]), score_type="BALD-PO")  # [N,]
    return obtain_scores(first_term, second_term, third_term, fourth_term, mode)  # [N,]


def bald_po_from_probs(probs: Tensor, mode: str) -> Tensor:
    """
    See bald_from_logprobs.

    Arguments:
        probs: Tensor[float], [N, K, Cl]

    Returns:
        Tensor[float], [N,]
    """
    first_term  = po_entropy_from_probs(probs, 'Exp_Entropy_unobs_par')
    second_term = po_entropy_from_probs(probs, 'Exp_par_Entropy_unobs')
    third_term  = po_entropy_from_probs(probs, 'Exp_unobs_Entropy_par')
    fourth_term = po_entropy_from_probs(probs, 'Exp_unobs_par_Entropy')

    #scores = first_term - second_term + third_term - fourth_term  # [N,]
    #scores = check(scores, math.log(probs.shape[-1]), score_type="BALD-PO")  # [N,]
    return obtain_scores(first_term, second_term, third_term, fourth_term, mode)  # [N,]
